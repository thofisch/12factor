{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nThese guidelines are based on the \nTwelve-Factor App\n, and adapted to how we recommend building services and applications ready for the cloud.\n\n\n\n\nCloud-native = cloud-ready = cloud-friendly\n\n\nWithin the context of these guildelines these terms are taking to mean the same, and is all about bringing cloud-centric best practices to software and IT generally, whether that be in the cloud or on premises.\n\n\nThe key takeaway: \ncloud-native\n is more than \ncloud-only\n.\n\n\n\n\nCloud-native Applications\n\n\nA cloud-native application is designed and implemented to run on a Platform-as-a-Service (PaaS) installation and to embrace horizontal elastic scaling. The platform hides infrastructure details from the application developer.\n\n\nAdhering to the following guidelines will help embrace elastic scalability, statelessness, and treating everything as a service, enabling development teams to built applications that can scale and be deployed rapidly:\n\n\n\n\nOne Codebase, One Application\n \n one codebase tracked in a version control system, many deploys.\n\n\nAPI First\n \n gives the ability to work against public contracts without interfering with internal development processes.\n\n\nBuild, Release, Run\n \n strictly separate build, release and run stages.\n\n\nDependency Management\n \n explicitly declare and isolate dependencies.\n\n\nConfiguration\n \n store configuration in the environment.\n\n\nEnvironment Parity\n \n keep all environments (development, staging, and production) as similar as possible.\n\n\nStateless Processes\n \n execute the application as one or more stateless processes.\n\n\nPort Binding\n \n export services via port binding.\n\n\nDisposability\n \n maximize robustness with fast startup and graceful shutdown.\n\n\nBacking Services\n \n treat backing services as attached resources.\n\n\nLogs\n \n treat logs as event streams.\n\n\nAdministrative Processes\n \n run admin/management tasks as one-off processes.\n\n\nConcurrency\n \n scale out via the process model.\n\n\nTelemetry\n \n you cannot control what you cannot measure.\n\n\nAuthentication and Authorization\n \n security should never be an afterthought.\n\n\n\n\nAs many of these factors feed into each other, following one factor makes it easier to follow another, and so on.\n\n\nDon't think about these guidelines as an all-or-nothing approach, but rather learn where and when to compromise when planning and implementing cloud-native applications. \nshould we allow for thought?\n\n\nWhen you\u2019re building a new application, force a decision as to why you should not build your application in a cloud-native way.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#introduction", 
            "text": "These guidelines are based on the  Twelve-Factor App , and adapted to how we recommend building services and applications ready for the cloud.   Cloud-native = cloud-ready = cloud-friendly  Within the context of these guildelines these terms are taking to mean the same, and is all about bringing cloud-centric best practices to software and IT generally, whether that be in the cloud or on premises.  The key takeaway:  cloud-native  is more than  cloud-only .", 
            "title": "Introduction"
        }, 
        {
            "location": "/#cloud-native-applications", 
            "text": "A cloud-native application is designed and implemented to run on a Platform-as-a-Service (PaaS) installation and to embrace horizontal elastic scaling. The platform hides infrastructure details from the application developer.  Adhering to the following guidelines will help embrace elastic scalability, statelessness, and treating everything as a service, enabling development teams to built applications that can scale and be deployed rapidly:   One Codebase, One Application    one codebase tracked in a version control system, many deploys.  API First    gives the ability to work against public contracts without interfering with internal development processes.  Build, Release, Run    strictly separate build, release and run stages.  Dependency Management    explicitly declare and isolate dependencies.  Configuration    store configuration in the environment.  Environment Parity    keep all environments (development, staging, and production) as similar as possible.  Stateless Processes    execute the application as one or more stateless processes.  Port Binding    export services via port binding.  Disposability    maximize robustness with fast startup and graceful shutdown.  Backing Services    treat backing services as attached resources.  Logs    treat logs as event streams.  Administrative Processes    run admin/management tasks as one-off processes.  Concurrency    scale out via the process model.  Telemetry    you cannot control what you cannot measure.  Authentication and Authorization    security should never be an afterthought.   As many of these factors feed into each other, following one factor makes it easier to follow another, and so on.  Don't think about these guidelines as an all-or-nothing approach, but rather learn where and when to compromise when planning and implementing cloud-native applications.  should we allow for thought?  When you\u2019re building a new application, force a decision as to why you should not build your application in a cloud-native way.", 
            "title": "Cloud-native Applications"
        }, 
        {
            "location": "/codebase/", 
            "text": "One Codebase, One Application\n\n\nA codebase is a source code repository.\n\n\nA cloud-native application always consist of a single codebase that is tracked in a version control system.\n\n\nThe single codebase for an application is used to produce \nimmutable releases\n for different \nenvironments\n.\n\n\nOne codebase, one application\n does not mean that it is not allowed to share code across multiple applications; it just means that the shared code is yet another codebase.\n\n\nViolations\n\n\n\n\nIf there are multiple codebases, it is not an applications \u2013 it is a distributed system.\n\n\nMultiple applications sharing the same codebase is a violation of this principle. A solution is to factor shared code into libraries which can be included as depedencies.\n\n\nApplications made of up multiple source code repositories, are nearly impossible to automate.\n\n\n\n\n\n\nConway's Law\n\n\nHaving multiple applications within a single codebase is often a sign that multiple teams are maintaining a single codebase. \n\n\nTake care that poor organization and lack of discipline among teams does not lead to the same dysfunction or lack of discipline in the code.\n\n\nWhen looking at your application and deciding on opportunities to reorganize the codebase and teams onto smaller products, you may find that one or more of the multiple codebases contributing to your application could be split out and converted into a microservice or API that can be reused by multiple applications.\n\n\n\n\nKubernetes\n\n\n\n\nShould we touch upon Container Patterns here?\n\n\nShould we discuss Kubernetes manifests/GitOps?", 
            "title": "One CodeBase, One Application"
        }, 
        {
            "location": "/codebase/#one-codebase-one-application", 
            "text": "A codebase is a source code repository.  A cloud-native application always consist of a single codebase that is tracked in a version control system.  The single codebase for an application is used to produce  immutable releases  for different  environments .  One codebase, one application  does not mean that it is not allowed to share code across multiple applications; it just means that the shared code is yet another codebase.", 
            "title": "One Codebase, One Application"
        }, 
        {
            "location": "/codebase/#violations", 
            "text": "If there are multiple codebases, it is not an applications \u2013 it is a distributed system.  Multiple applications sharing the same codebase is a violation of this principle. A solution is to factor shared code into libraries which can be included as depedencies.  Applications made of up multiple source code repositories, are nearly impossible to automate.    Conway's Law  Having multiple applications within a single codebase is often a sign that multiple teams are maintaining a single codebase.   Take care that poor organization and lack of discipline among teams does not lead to the same dysfunction or lack of discipline in the code.  When looking at your application and deciding on opportunities to reorganize the codebase and teams onto smaller products, you may find that one or more of the multiple codebases contributing to your application could be split out and converted into a microservice or API that can be reused by multiple applications.", 
            "title": "Violations"
        }, 
        {
            "location": "/codebase/#kubernetes", 
            "text": "Should we touch upon Container Patterns here?  Should we discuss Kubernetes manifests/GitOps?", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/api-first/", 
            "text": "API First\n\n\nRegardless of the type of application, the ultimate goal might very well be to have that application be a participant in an ecosystem of services.\n\n\nEven if, at the outset, the service is not a part of a larger ecosystem, the discipline of starting development at the API level still pays enough dividends to make it worth the effort.\n\n\nWhen building cloud-native applications, chances are another team in the organization starts building services relying on your code. Soon multiple teams are building services with horizontal dependencies that are all on a different release cadence.\n\n\nTo avoid integration failures, API first gives teams the ability to work against each other\u2019s public contracts without interfering with internal development processes.\n\n\nIf we treat and develop all applications as \nbacking services\n, and designed them API-first; the system will be free to grow, adapt to new load and demand, and accommodate new consumers without having to re-architect yet another closed system.\n\n\nLink to our in-depth API First guidelines", 
            "title": "API First"
        }, 
        {
            "location": "/api-first/#api-first", 
            "text": "Regardless of the type of application, the ultimate goal might very well be to have that application be a participant in an ecosystem of services.  Even if, at the outset, the service is not a part of a larger ecosystem, the discipline of starting development at the API level still pays enough dividends to make it worth the effort.  When building cloud-native applications, chances are another team in the organization starts building services relying on your code. Soon multiple teams are building services with horizontal dependencies that are all on a different release cadence.  To avoid integration failures, API first gives teams the ability to work against each other\u2019s public contracts without interfering with internal development processes.  If we treat and develop all applications as  backing services , and designed them API-first; the system will be free to grow, adapt to new load and demand, and accommodate new consumers without having to re-architect yet another closed system.  Link to our in-depth API First guidelines", 
            "title": "API First"
        }, 
        {
            "location": "/build-release-run/", 
            "text": "Build, Release, Run\n\n\nGoing from \ndesign/code\n to running in production, could take as little as a few minutes, when a mature CI/CD pipeline is in place. Each of the following deployment stages is isolated and occurs separately:\n\n\n\n\nA single codebase is taken through the build process to produce a compiled/binary \nimmutable build artifact\n.\n\n\nThis immutable build artifact is then merged with configuration information that is external to the application to produce an \nimmutable release\n.\n\n\nThe immutable release is then delivered to an environment (development, QA, production, etc.) and run.\n\n\n\n\nShould we be more Kubernetes specific\n\n\nBuild\n\n\nThe build stage is where a code repository is converted into a versioned, immutable build artifact.\n\n\nDependencies\n are fetched and bundled into the immutable build artifact.\n\n\nThe immutable build artifact could a ZIP file, a binary executable, or a container image.\n\n\nBuilds are created by a CI server, and there is a one-to-many relationship between builds and deployments, so a single build should be able to be released or deployed to any number of environments, and each of those unmodified builds should work as expected.\n\n\nThe immutability of this artifact coupled with \nenvironment parity\n give you confidence that your applications will work in production if it worked in QA.\n\n\nRelease\n\n\nThe release is done by pushing to your \ncloud/PaaS/Kubernetes\n environment.\n\n\nThe output of the build stage is combined with environment \nconfiguration\n information to produce another immutable artifact, a release.\n\n\nReleases must to be unique and identifiable, and every release should ideally be tagged with some kind of unique ID, such as a timestamp or an auto-incrementing number.\n\n\nConsidering the one-to-many relationship between builds and releases, does it makes sense that releases should be tagged with the build ID (build+env)? We \ncheat\n a bit with our manifests being copied as build artifacts and then \ntransformed\n during the release phase. I don't like that we cannot pull down a complete release without looking through VSTS (are variables versioned, etc)\n\n\n\n\nDanger\n\n\nA lot of problems can arise from the inability to reproduce a release as it appeared at one point in the past.\n\n\nHaving a separate release phase, and storing those artifacts, enable easy rollback \nbut do we want it?\n and historical auditing.\n\n\n\n\nRun\n\n\nThe run phase is done by the \ncloud provider/PaaS/Kubernetes\n.\n\n\nWhen an application is running, the platform is responsible for keeping it alive, monitoring its health, aggregating its logs, as well as a lot of other administrative tasks like dynamic scaling and fault tolerance.\n\n\n\n\nRun locally\n\n\nIn order for developers to feel unhindered while working on cloud-native applications, it is \nalmost\n always worth the extra effort of ensuring that developers can run an application locally on their workstations, while still allowing it to be deployed via CD pipeline.\n\n\nUsing Docker and/or Minikube help create near-clones of production systems.", 
            "title": "Build, Release, Run"
        }, 
        {
            "location": "/build-release-run/#build-release-run", 
            "text": "Going from  design/code  to running in production, could take as little as a few minutes, when a mature CI/CD pipeline is in place. Each of the following deployment stages is isolated and occurs separately:   A single codebase is taken through the build process to produce a compiled/binary  immutable build artifact .  This immutable build artifact is then merged with configuration information that is external to the application to produce an  immutable release .  The immutable release is then delivered to an environment (development, QA, production, etc.) and run.   Should we be more Kubernetes specific", 
            "title": "Build, Release, Run"
        }, 
        {
            "location": "/build-release-run/#build", 
            "text": "The build stage is where a code repository is converted into a versioned, immutable build artifact.  Dependencies  are fetched and bundled into the immutable build artifact.  The immutable build artifact could a ZIP file, a binary executable, or a container image.  Builds are created by a CI server, and there is a one-to-many relationship between builds and deployments, so a single build should be able to be released or deployed to any number of environments, and each of those unmodified builds should work as expected.  The immutability of this artifact coupled with  environment parity  give you confidence that your applications will work in production if it worked in QA.", 
            "title": "Build"
        }, 
        {
            "location": "/build-release-run/#release", 
            "text": "The release is done by pushing to your  cloud/PaaS/Kubernetes  environment.  The output of the build stage is combined with environment  configuration  information to produce another immutable artifact, a release.  Releases must to be unique and identifiable, and every release should ideally be tagged with some kind of unique ID, such as a timestamp or an auto-incrementing number.  Considering the one-to-many relationship between builds and releases, does it makes sense that releases should be tagged with the build ID (build+env)? We  cheat  a bit with our manifests being copied as build artifacts and then  transformed  during the release phase. I don't like that we cannot pull down a complete release without looking through VSTS (are variables versioned, etc)   Danger  A lot of problems can arise from the inability to reproduce a release as it appeared at one point in the past.  Having a separate release phase, and storing those artifacts, enable easy rollback  but do we want it?  and historical auditing.", 
            "title": "Release"
        }, 
        {
            "location": "/build-release-run/#run", 
            "text": "The run phase is done by the  cloud provider/PaaS/Kubernetes .  When an application is running, the platform is responsible for keeping it alive, monitoring its health, aggregating its logs, as well as a lot of other administrative tasks like dynamic scaling and fault tolerance.   Run locally  In order for developers to feel unhindered while working on cloud-native applications, it is  almost  always worth the extra effort of ensuring that developers can run an application locally on their workstations, while still allowing it to be deployed via CD pipeline.  Using Docker and/or Minikube help create near-clones of production systems.", 
            "title": "Run"
        }, 
        {
            "location": "/dependency-management/", 
            "text": "Dependency Management\n\n\nA cloud-native application must be self-contained, where everything needed to run the application is contained within a single build artifact. Applications can no longer assume that a server will have everything they need. Instead, applications need to bring their dependencies with them.\n\n\nE.g., never rely on the implicit existence of system-wide packages, or facilities like e.g. the Global Assembly Cache.\n\n\n\n\nRepeatable deployments\n\n\nManaging application dependencies is all about repeatable deployments.\n\n\nIf it isn't automated, nothing about the runtime, into which an application is deployed, should be assumed.\n\n\n\n\nKubernetes\n\n\nFortunately, it is relatively easy for the container to include all of the dependencies on which the application relies, and provide a reasonably isolated environment in which the container runs. \n(container environments are not completely isolated, but for most situations, they are Good Enough.)\n\n\nFor applications that are modularized and depend on other components, Kubernetes provides a way to combine all of these pieces into a single Pod, for an environment that encapsulates those pieces appropriately.", 
            "title": "Dependency Management"
        }, 
        {
            "location": "/dependency-management/#dependency-management", 
            "text": "A cloud-native application must be self-contained, where everything needed to run the application is contained within a single build artifact. Applications can no longer assume that a server will have everything they need. Instead, applications need to bring their dependencies with them.  E.g., never rely on the implicit existence of system-wide packages, or facilities like e.g. the Global Assembly Cache.   Repeatable deployments  Managing application dependencies is all about repeatable deployments.  If it isn't automated, nothing about the runtime, into which an application is deployed, should be assumed.", 
            "title": "Dependency Management"
        }, 
        {
            "location": "/dependency-management/#kubernetes", 
            "text": "Fortunately, it is relatively easy for the container to include all of the dependencies on which the application relies, and provide a reasonably isolated environment in which the container runs.  (container environments are not completely isolated, but for most situations, they are Good Enough.)  For applications that are modularized and depend on other components, Kubernetes provides a way to combine all of these pieces into a single Pod, for an environment that encapsulates those pieces appropriately.", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/configuration/", 
            "text": "Configuration\n\n\nAn application should be completely independent from its configuration, allowing it to move to another environment without having to touch the source code.\n\n\nConfiguration refers to any value that can vary across deployments/environments (e.g., developer workstation, QA, and production), and may could include:\n\n\n\n\nURLs and other information about \nbacking services\n.\n\n\nConnection strings.\n\n\nCredentials.\n\n\nInformation that might normally be bundled in configuration files (.NET applications traditionally get configuration from XML-based config files).\n\n\n\n\nConfiguration does not include internal information that is part of the application itself, so values that remains the same across all deployments is not considered configuration.\n\n\nCredentials are extremely sensitive information and have absolutely no business in a codebase.\n\n\nExternalizing Configuration\n\n\nExternal configuration allows deploying \nimmutable builds\n to multiple environments via CD pipelines and helps maintain \nenvironment parity\n.\n\n\nThe easiest and preferred options is to use environment variables to externalize configuration. As this works well both locally as well as \nfor most cloud providers/PaaS/Kubernetes\n.\n\n\nAnother option for externalizing configuration is to use a server product designed to expose configuration.\n\n\nWhen externalizing configuration, it should be possible to secure data changes as well as obtain a history of who made what changes and when.\n\n\nKubernetes\n\n\nKubernetes enables you to specify environment variables in manifests, but as these manifests are part of the codebase, that is not a complete solution.\n\n\nOne options is to transform the manifests during build/release. \nhmm, don't like this now I have to describe it...\n\n\nAnother option is to specify the environment variables using ConfigMaps and Secrets, which can be kept separate from the application. For example, you might define:\n\n\n...\n\n\nspec\n:\n\n  \ncontainers\n:\n\n  \n-\n \nname\n:\n \ncontainer\n\n    \nimage\n:\n \nimage:tag\n\n    \nenv\n:\n\n    \n-\n \nname\n:\n \nSECRET_USERNAME\n\n      \nvalueFrom\n:\n\n        \nsecretKeyRef\n:\n\n          \nname\n:\n \nmysecret\n\n          \nkey\n:\n \nusername\n\n    \n-\n \nname\n:\n \nSECRET_PASSWORD\n\n      \nvalueFrom\n:\n\n        \nsecretKeyRef\n:\n\n          \nname\n:\n \nmysecret\n\n          \nkey\n:\n \npassword\n\n    \n-\n \nname\n:\n \nEXTERNAL_SERVICE_URL\n\n      \nvalueFrom\n:\n\n        \nconfigMapKeyRef\n:\n\n          \nname\n:\n \nmyconfig\n\n          \nkey\n:\n \nexternal.service\n\n\n...\n\n\n\n\n\nThe Pod then receives the environment variables, \nSECRET_USERNAME\n, \nSECRET_PASSWORD\n, and \nEXTERNAL_SERVICE_URL\n, the first two from the referenced Kubernetes Secrets, and the third from a Kubernetes ConfigMap.\n\n\n\n\nEnvironment Variables\n\n\nEnvironment variables are not necessarily safe for their own reasons. For example, if an app crashes, it may save all of the environment variables to a log or even transmit them to another service. \nhttps://diogomonica.com/2017/03/27/why-you-shouldnt-use-env-variables-for-secret-data/\n. Consider a server product then.", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuration/#configuration", 
            "text": "An application should be completely independent from its configuration, allowing it to move to another environment without having to touch the source code.  Configuration refers to any value that can vary across deployments/environments (e.g., developer workstation, QA, and production), and may could include:   URLs and other information about  backing services .  Connection strings.  Credentials.  Information that might normally be bundled in configuration files (.NET applications traditionally get configuration from XML-based config files).   Configuration does not include internal information that is part of the application itself, so values that remains the same across all deployments is not considered configuration.  Credentials are extremely sensitive information and have absolutely no business in a codebase.", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuration/#externalizing-configuration", 
            "text": "External configuration allows deploying  immutable builds  to multiple environments via CD pipelines and helps maintain  environment parity .  The easiest and preferred options is to use environment variables to externalize configuration. As this works well both locally as well as  for most cloud providers/PaaS/Kubernetes .  Another option for externalizing configuration is to use a server product designed to expose configuration.  When externalizing configuration, it should be possible to secure data changes as well as obtain a history of who made what changes and when.", 
            "title": "Externalizing Configuration"
        }, 
        {
            "location": "/configuration/#kubernetes", 
            "text": "Kubernetes enables you to specify environment variables in manifests, but as these manifests are part of the codebase, that is not a complete solution.  One options is to transform the manifests during build/release.  hmm, don't like this now I have to describe it...  Another option is to specify the environment variables using ConfigMaps and Secrets, which can be kept separate from the application. For example, you might define:  ...  spec : \n   containers : \n   -   name :   container \n     image :   image:tag \n     env : \n     -   name :   SECRET_USERNAME \n       valueFrom : \n         secretKeyRef : \n           name :   mysecret \n           key :   username \n     -   name :   SECRET_PASSWORD \n       valueFrom : \n         secretKeyRef : \n           name :   mysecret \n           key :   password \n     -   name :   EXTERNAL_SERVICE_URL \n       valueFrom : \n         configMapKeyRef : \n           name :   myconfig \n           key :   external.service  ...   The Pod then receives the environment variables,  SECRET_USERNAME ,  SECRET_PASSWORD , and  EXTERNAL_SERVICE_URL , the first two from the referenced Kubernetes Secrets, and the third from a Kubernetes ConfigMap.   Environment Variables  Environment variables are not necessarily safe for their own reasons. For example, if an app crashes, it may save all of the environment variables to a log or even transmit them to another service.  https://diogomonica.com/2017/03/27/why-you-shouldnt-use-env-variables-for-secret-data/ . Consider a server product then.", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/environment-parity/", 
            "text": "Environment Parity\n\n\nKeep all of the environments as similar as possible will give the confidence that the application will work everywhere.\n\n\nWhen environments differ, we lose our ability to accurately predict how the application is going to behave in production.\n\n\n\n\nEnvironment drift\n\n\nEnvironment drift may a occurs  because:\n\n\n\n\nthe shared development environment has a different scaling and reliability profile than QA, which is also different than production.\n\n\nthe database drivers used in dev and QA are different than production.\n\n\nsecurity rules, firewalls, and other configuration settings are also different.\n\n\nsome people have the ability to deploy to some environments, but not others.\n\n\n\n\n\n\nThe opportunities for creating a gap between environments are nearly infinite; \ntime\n, \npeople\n, and \nresources\n, are the most common culprits. In order to decrease gaps between environments:\n\n\n\n\n\n\nReduce the time gap from check-in to production\"\n\n\nEvery commit should end up in production after the amount of time it takes to run all tests, vet th change against all integration suites, and deploy to pre-production environments.\n\n\n\n\n\n\nAutomate deployments\n\n\nHumans should never be deploying applications, at least not to any environment other than their own workstations or labs.\n\n\n\n\n\n\nAccept no excuses as to why resource types should differ across environments\n\n\nThere is almost an infinite set of tools available, from granting local instances of databases, to using Docker on developer workstations, to provisioning isolated resources in the \ncloud/PaaS/Kubernetes\n.\n\n\n\n\n\n\nEvery decision that increases the functional gap between environments must be flagged and questioned. Resist the urge to mitigate this problem by allowing environments to differ.\n\n\nKubernetes\n\n\n\n\nKubernetes namespaces enables (theoretically) running code on the same actual cluster against the same actual systems while still keeping environments separate.\n\n\nAlso use tools such as Minikube to create near-clones of production systems.\n\n\nrolling updates", 
            "title": "Environment Parity"
        }, 
        {
            "location": "/environment-parity/#environment-parity", 
            "text": "Keep all of the environments as similar as possible will give the confidence that the application will work everywhere.  When environments differ, we lose our ability to accurately predict how the application is going to behave in production.   Environment drift  Environment drift may a occurs  because:   the shared development environment has a different scaling and reliability profile than QA, which is also different than production.  the database drivers used in dev and QA are different than production.  security rules, firewalls, and other configuration settings are also different.  some people have the ability to deploy to some environments, but not others.    The opportunities for creating a gap between environments are nearly infinite;  time ,  people , and  resources , are the most common culprits. In order to decrease gaps between environments:    Reduce the time gap from check-in to production\"  Every commit should end up in production after the amount of time it takes to run all tests, vet th change against all integration suites, and deploy to pre-production environments.    Automate deployments  Humans should never be deploying applications, at least not to any environment other than their own workstations or labs.    Accept no excuses as to why resource types should differ across environments  There is almost an infinite set of tools available, from granting local instances of databases, to using Docker on developer workstations, to provisioning isolated resources in the  cloud/PaaS/Kubernetes .    Every decision that increases the functional gap between environments must be flagged and questioned. Resist the urge to mitigate this problem by allowing environments to differ.", 
            "title": "Environment Parity"
        }, 
        {
            "location": "/environment-parity/#kubernetes", 
            "text": "Kubernetes namespaces enables (theoretically) running code on the same actual cluster against the same actual systems while still keeping environments separate.  Also use tools such as Minikube to create near-clones of production systems.  rolling updates", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/stateless-processes/", 
            "text": "Stateless Processes\n\n\nApplications should execute as a single, stateless process.\n\n\nIt is not that state cannot exist; it is that it must \nnot\n be maintained within the application, meaning that all \nlong-lasting\n state must be external to the application, provided by \nbacking services\n.\n\n\n\n\nThe filesystem is not a backing service\n\n\nYou cannot consider files a means by which applications can share data \nor can you?\n. Disks in the cloud are ephemeral and, in some cases, even read-only.\n\n\n\n\nShare-Nothing\n\n\nProcesses are highly \ndisposable\n, so they may come and go, scale horizontally and vertically. So anything shared among processes could also vanish at a moment's notice and without warning, potentially causing a cascading failure.\n\n\nIf processes need to share data (e.g., session state), again that data should be externalized and made available through a backing service.\n\n\n\n\nAvoid in-memory data caching\n\n\nBe mindful that processes need to start and stop quickly, so taking a long time to fill and in-memory cache during application startup violates this principle (\nDisposability\n).\n\n\nTake advantage of third-party caching products (like Redis) that are designed to act as a backing service for your applications.\n\n\n\n\nKubernetes\n\n\nShould we mention StatefulSets (I'm thinking no)?", 
            "title": "Stateless Processes"
        }, 
        {
            "location": "/stateless-processes/#stateless-processes", 
            "text": "Applications should execute as a single, stateless process.  It is not that state cannot exist; it is that it must  not  be maintained within the application, meaning that all  long-lasting  state must be external to the application, provided by  backing services .   The filesystem is not a backing service  You cannot consider files a means by which applications can share data  or can you? . Disks in the cloud are ephemeral and, in some cases, even read-only.", 
            "title": "Stateless Processes"
        }, 
        {
            "location": "/stateless-processes/#share-nothing", 
            "text": "Processes are highly  disposable , so they may come and go, scale horizontally and vertically. So anything shared among processes could also vanish at a moment's notice and without warning, potentially causing a cascading failure.  If processes need to share data (e.g., session state), again that data should be externalized and made available through a backing service.   Avoid in-memory data caching  Be mindful that processes need to start and stop quickly, so taking a long time to fill and in-memory cache during application startup violates this principle ( Disposability ).  Take advantage of third-party caching products (like Redis) that are designed to act as a backing service for your applications.", 
            "title": "Share-Nothing"
        }, 
        {
            "location": "/stateless-processes/#kubernetes", 
            "text": "Should we mention StatefulSets (I'm thinking no)?", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/port-binding/", 
            "text": "Port Binding\n\n\nCloud-native applications export services via port binding.\n\n\nWe cannot relying on some kind of application server, like Microsoft Internet Information Server (IIS), to host our application. We must bundle an embedded server, because a cloud-native application is self-contained (see \nDepedency Management\n), and never injected into any kind of external application server.\n\n\nUsing externalized \nconfiguration\n allow applications to do port binding at runtime, making it easy to act as a backing service for other applications, and supports environment-specific port binding without having to change any code.\n\n\nKubernetes\n\n\nAnything specific we need to mention in regards to Kubernetes?", 
            "title": "Port Binding"
        }, 
        {
            "location": "/port-binding/#port-binding", 
            "text": "Cloud-native applications export services via port binding.  We cannot relying on some kind of application server, like Microsoft Internet Information Server (IIS), to host our application. We must bundle an embedded server, because a cloud-native application is self-contained (see  Depedency Management ), and never injected into any kind of external application server.  Using externalized  configuration  allow applications to do port binding at runtime, making it easy to act as a backing service for other applications, and supports environment-specific port binding without having to change any code.", 
            "title": "Port Binding"
        }, 
        {
            "location": "/port-binding/#kubernetes", 
            "text": "Anything specific we need to mention in regards to Kubernetes?", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/disposability/", 
            "text": "Disposability\n\n\nWhen a cloud-native application is disposable, it means they can be started or stopped rapidly.\n\n\nAn application cannot scale, deploy, release, or recover rapidly, if it cannot start rapidly and shut down gracefully.\n\n\nTo take full advantage of the platform, build applications that not only are aware of this, but also embrace it.\n\n\n\n\nSlow startup time\n\n\nBringing up an application that takes minutes to get into a steady state:\n\n\n\n\ncould mean hundreds or thousands of requests potentially getting denied\n\n\nmight trigger alerts or warnings as the application fails its health checks\n\n\neven prevent the application from starting at all\n\n\n\n\n\n\n\n\nGraceful shut down\n\n\nWhen shut down is not done quickly and gracefully:\n\n\n\n\ncan fail bringing the application back up again after failure\n\n\nruns the risk of failing to dispose of resources, which could corrupt data\n\n\n\n\n\n\n\n\nTip\n\n\nLong-running activities during application startup (populating a cache or preparing other runtime dependencies) must find another way of dealing with these activities. E.g., the cache could be externalized into a backing service so that your application can go up and down rapidly without performing front-loaded operations.\n\n\n\n\nKubernetes\n\n\nIt seems like this principle was tailor made for containers and Kubernetes-based applications. The idea that processes should be disposable means that at any time, an application can die and the user won\u2019t be affected, either because there are others to take its place, because it\u2019ll start right up again, or both.\n\n\nContainers are built on this principle, of course, and Kubernetes structures that manage multiple instances and maintain a certain level of availability even in the face of problems, such as ReplicaSets, complete the picture.", 
            "title": "Disposability"
        }, 
        {
            "location": "/disposability/#disposability", 
            "text": "When a cloud-native application is disposable, it means they can be started or stopped rapidly.  An application cannot scale, deploy, release, or recover rapidly, if it cannot start rapidly and shut down gracefully.  To take full advantage of the platform, build applications that not only are aware of this, but also embrace it.   Slow startup time  Bringing up an application that takes minutes to get into a steady state:   could mean hundreds or thousands of requests potentially getting denied  might trigger alerts or warnings as the application fails its health checks  even prevent the application from starting at all     Graceful shut down  When shut down is not done quickly and gracefully:   can fail bringing the application back up again after failure  runs the risk of failing to dispose of resources, which could corrupt data     Tip  Long-running activities during application startup (populating a cache or preparing other runtime dependencies) must find another way of dealing with these activities. E.g., the cache could be externalized into a backing service so that your application can go up and down rapidly without performing front-loaded operations.", 
            "title": "Disposability"
        }, 
        {
            "location": "/disposability/#kubernetes", 
            "text": "It seems like this principle was tailor made for containers and Kubernetes-based applications. The idea that processes should be disposable means that at any time, an application can die and the user won\u2019t be affected, either because there are others to take its place, because it\u2019ll start right up again, or both.  Containers are built on this principle, of course, and Kubernetes structures that manage multiple instances and maintain a certain level of availability even in the face of problems, such as ReplicaSets, complete the picture.", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/backing-services/", 
            "text": "Backing Services\n\n\nA backing service is any service on which your application relies for its functionality that is not part of the core application.\n\n\nExamples include databases, external storage, message queues, SMTP services, caching services, and any number of other types of service, including services that perform line-of-business functionality or security.\n\n\n\n\nUse configuration\n\n\nThe binding of an application to its backing services should be done via \nconfiguration\n.\n\n\n\n\nResource Binding\n\n\nA bound resource is really just a means of connecting your application to a backing service (e.g., a connection string).\n\n\nThe code should make no distinction between local (SQLExpress) and third party services (Amazon RDS)). To the application, both are attached resources, accessed via a URL or other locator/credentials stored in the configuration.\n\n\nIt should be possible to attach and detach backing services from an application at will, without re-deploying the application. This means that there is never a line of code in your application that tightly couples the application to a specific backing service.\n\n\n\n\nCircuit Breakers\n\n\nIs a pattern (supported by libraries and cloud offerings) that will allow the application to simply stop communicating with misbehaving backing services, providing a fallback or failsafe path.\n\n\n\n\nKubernetes\n\n\nNote that when changing configuration, even though no changes are made the code (or even the container image) you will most likely need to replace the Pod.", 
            "title": "Backing Services"
        }, 
        {
            "location": "/backing-services/#backing-services", 
            "text": "A backing service is any service on which your application relies for its functionality that is not part of the core application.  Examples include databases, external storage, message queues, SMTP services, caching services, and any number of other types of service, including services that perform line-of-business functionality or security.   Use configuration  The binding of an application to its backing services should be done via  configuration .", 
            "title": "Backing Services"
        }, 
        {
            "location": "/backing-services/#resource-binding", 
            "text": "A bound resource is really just a means of connecting your application to a backing service (e.g., a connection string).  The code should make no distinction between local (SQLExpress) and third party services (Amazon RDS)). To the application, both are attached resources, accessed via a URL or other locator/credentials stored in the configuration.  It should be possible to attach and detach backing services from an application at will, without re-deploying the application. This means that there is never a line of code in your application that tightly couples the application to a specific backing service.   Circuit Breakers  Is a pattern (supported by libraries and cloud offerings) that will allow the application to simply stop communicating with misbehaving backing services, providing a fallback or failsafe path.", 
            "title": "Resource Binding"
        }, 
        {
            "location": "/backing-services/#kubernetes", 
            "text": "Note that when changing configuration, even though no changes are made the code (or even the container image) you will most likely need to replace the Pod.", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/logs/", 
            "text": "Logs\n\n\nTreat logs as event streams, i.e., a time-ordered sequence of events emitted from the application.\n\n\nA cloud-native application never concerns itself with routing or storage of its output stream, but simply writes all of its log entries to \nstdout\n and \nstderr\n.\n\n\nDecoupling the knowledge of log storage, processing, and analysis from the code, simplifies the codebase, and allows changing the way in which logs are store and processed without modifying the application, and helps with elastic scalability.\n\n\nThe aggregation, processing, and storage of logs is a non-functional requirement that is handled by the \ncloud provider/PaaS/Kubernetes\n, or some other tool running in concert with the platform. \n\n\nWe rely on industry-standard tools and stacks to deal with logs, such as FluentD, ElasticSearch, and Kibana.\n\n\nMaybe a section about structured logging", 
            "title": "Logs"
        }, 
        {
            "location": "/logs/#logs", 
            "text": "Treat logs as event streams, i.e., a time-ordered sequence of events emitted from the application.  A cloud-native application never concerns itself with routing or storage of its output stream, but simply writes all of its log entries to  stdout  and  stderr .  Decoupling the knowledge of log storage, processing, and analysis from the code, simplifies the codebase, and allows changing the way in which logs are store and processed without modifying the application, and helps with elastic scalability.  The aggregation, processing, and storage of logs is a non-functional requirement that is handled by the  cloud provider/PaaS/Kubernetes , or some other tool running in concert with the platform.   We rely on industry-standard tools and stacks to deal with logs, such as FluentD, ElasticSearch, and Kibana.  Maybe a section about structured logging", 
            "title": "Logs"
        }, 
        {
            "location": "/admin-processes/", 
            "text": "Administrative Processes\n\n\nStateless processes\n runs regular business applications (such as handling web requests).\n\n\nSeparately, developers will often wish to do one-off administrative or maintenance tasks for the application, such as:\n\n\n\n\nDatabase migrations\n\n\nRunning timed scripts, such as a nightly batch job or hourly import\n\n\nRunning a one-off job that executes custom code only once\n\n\n\n\nAll of which should probably not be a part of the regular business application. Always consider whether an administrative process is what you want, or whether a different design or architecture would better suit your needs.\n\n\n\n\n\n\nAvoid timers\n\n\nInternalized timers that perform batch operations may look appealing on the surface, but it might not scale when there are multiple instances of the application running. If they\u2019re all performing the same batch operation, this might also lead to corrupt or duplicate data.\n\n\n\n\n\n\nUse telemetry\n\n\nDo not rely on interactive shells, remote desktop, ssh, and even kubectl for in-process introspection, instead use \nTelemetry\n to cover more aspects of application monitoring.\n\n\n\n\n\n\nTimed administrative batch processes\n\n\nConsider exposing a RESTful endpoint that can be used to invoke ad hoc functionality, or alternatively, extract the batch-related code from the main application and create a separate application.\n\n\nEither will allow at-will invocation of timed functionality, but it moves the stimuli outside the application, while also solving the at most once execution problem that internal timers have on dynamically scaled instances.\n\n\n\n\n\n\nKubernetes\n\n\n\n\nJobs\n\n\nCron Jobs\n\n\nContainer Patterns\n\n\nAWS Lambda", 
            "title": "Administrative Processes"
        }, 
        {
            "location": "/admin-processes/#administrative-processes", 
            "text": "Stateless processes  runs regular business applications (such as handling web requests).  Separately, developers will often wish to do one-off administrative or maintenance tasks for the application, such as:   Database migrations  Running timed scripts, such as a nightly batch job or hourly import  Running a one-off job that executes custom code only once   All of which should probably not be a part of the regular business application. Always consider whether an administrative process is what you want, or whether a different design or architecture would better suit your needs.    Avoid timers  Internalized timers that perform batch operations may look appealing on the surface, but it might not scale when there are multiple instances of the application running. If they\u2019re all performing the same batch operation, this might also lead to corrupt or duplicate data.    Use telemetry  Do not rely on interactive shells, remote desktop, ssh, and even kubectl for in-process introspection, instead use  Telemetry  to cover more aspects of application monitoring.    Timed administrative batch processes  Consider exposing a RESTful endpoint that can be used to invoke ad hoc functionality, or alternatively, extract the batch-related code from the main application and create a separate application.  Either will allow at-will invocation of timed functionality, but it moves the stimuli outside the application, while also solving the at most once execution problem that internal timers have on dynamically scaled instances.", 
            "title": "Administrative Processes"
        }, 
        {
            "location": "/admin-processes/#kubernetes", 
            "text": "Jobs  Cron Jobs  Container Patterns  AWS Lambda", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/concurrency/", 
            "text": "Concurrency\n\n\nCloud-native applications should scale out using the process model.\n\n\nInstead of making a single big process even larger, you create multiple processes, and then distribute the load of the application among those processes.\n\n\nCloud providers/PaaS/Kubernetes\n have perfected this capability to the point where you can even configure rules that will dynamically scale the number of instances of your application based on load or other runtime telemetry available in a system.\n\n\nTake advantage of applications build as \ndisposable\n and \nstateless, share-nothing processes\n to take full advantage of horizontal scaling.\n\n\n\n\nAvoid vertical scaling\n\n\nThere was a time when, if an application reached the limit of its capacity, the solution was to increase its size.\n\n\nAdding CPUs, RAM, and other resources (virtual or physical) to a single monolithic application is typically frowned upon in civilized society these days.\n\n\n\n\nKubernetes\n\n\n\n\nHorizontal scaling is a key capability of Kubernetes", 
            "title": "Concurrency"
        }, 
        {
            "location": "/concurrency/#concurrency", 
            "text": "Cloud-native applications should scale out using the process model.  Instead of making a single big process even larger, you create multiple processes, and then distribute the load of the application among those processes.  Cloud providers/PaaS/Kubernetes  have perfected this capability to the point where you can even configure rules that will dynamically scale the number of instances of your application based on load or other runtime telemetry available in a system.  Take advantage of applications build as  disposable  and  stateless, share-nothing processes  to take full advantage of horizontal scaling.   Avoid vertical scaling  There was a time when, if an application reached the limit of its capacity, the solution was to increase its size.  Adding CPUs, RAM, and other resources (virtual or physical) to a single monolithic application is typically frowned upon in civilized society these days.", 
            "title": "Concurrency"
        }, 
        {
            "location": "/concurrency/#kubernetes", 
            "text": "Horizontal scaling is a key capability of Kubernetes", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/telemetry/", 
            "text": "Telemetry\n\n\nTelemetry should be an essential part of any cloud-native application.\n\n\nBuilding applications on your workstation lets you can inspect the inside of your application, execute a debugger, and perform other tasks that give you visibility deep within your applications and its behavior.\n\n\nThis is not possible with a cloud-native, because of a number of reasons:\n\n\n\n\nYour application instance might move with little or no warning.\n\n\nYou could start with one instance of your app, and a few minutes later, you might have hundreds of copies of your application running.\n\n\nYou will not have direct access to the running process.\n\n\n\n\nIf you cannot physically touch it or bang it with a hammer to coerce it into behaving, what kind of telemetry would you want?\n\n\nWhen it comes to monitoring your application, there are generally a few different categories of data:\n\n\n\n\nApplication Performance M;onitoring (APM)\n\n\nconsists of a stream of events that can be used by tools to keep tabs on how well your application is performing.\n\n\nThis is something that you are responsible for, specific to your application and standards.\n\n\nThe data used to supply APM dashboards is usually fairly generic and can come from multiple applications across multiple lines of business.\n\n\nAPM might provide you the average number of HTTP requests per second an application is processing.\n\n\n\n\n\n\nDomain-specific telemetry\n\n\nIs also up to you.\n\n\nThis refers to the stream of events and data that makes sense to your business that you can use for your own analytics and reporting.\n\n\nThis type of event stream is often fed into a \"big data\" system for warehousing, analysis, and forecasting.\n\n\ndomain-specific telemetry might tell you the number of widgets sold to people on iPads within the last 20 minutes.\n\n\n\n\n\n\nHealth and system logs\n\n\nFinally, health and system logs are something that should be provided by your cloud provider.\n\n\nThey make up a stream of events, such as application start, shutdown, scaling, web request tracing, and the results of periodic health checks.\n\n\n\n\n\n\n\n\nWhen planning your monitoring strategy, you need to take into account how much information you\u2019ll be aggregating, the rate at which it comes in, and how much of it you\u2019re going to store. If your application dynamically scales from 1 instance to 100, that can also result in a hundredfold increase in your log traffic.\n\n\nAuditing and monitoring cloud applications are often overlooked but are perhaps some of the most important things to plan and do properly for production deployments.\n\n\nGetting telemetry done right can mean the difference between success and failure in the cloud.\n\n\nKubernetes\n\n\n\n\nLiveliness/readiness probes\n\n\nPrometheus\n\n\nRED metrics (+ prebuild dashboards)", 
            "title": "Telemetry"
        }, 
        {
            "location": "/telemetry/#telemetry", 
            "text": "Telemetry should be an essential part of any cloud-native application.  Building applications on your workstation lets you can inspect the inside of your application, execute a debugger, and perform other tasks that give you visibility deep within your applications and its behavior.  This is not possible with a cloud-native, because of a number of reasons:   Your application instance might move with little or no warning.  You could start with one instance of your app, and a few minutes later, you might have hundreds of copies of your application running.  You will not have direct access to the running process.   If you cannot physically touch it or bang it with a hammer to coerce it into behaving, what kind of telemetry would you want?  When it comes to monitoring your application, there are generally a few different categories of data:   Application Performance M;onitoring (APM)  consists of a stream of events that can be used by tools to keep tabs on how well your application is performing.  This is something that you are responsible for, specific to your application and standards.  The data used to supply APM dashboards is usually fairly generic and can come from multiple applications across multiple lines of business.  APM might provide you the average number of HTTP requests per second an application is processing.    Domain-specific telemetry  Is also up to you.  This refers to the stream of events and data that makes sense to your business that you can use for your own analytics and reporting.  This type of event stream is often fed into a \"big data\" system for warehousing, analysis, and forecasting.  domain-specific telemetry might tell you the number of widgets sold to people on iPads within the last 20 minutes.    Health and system logs  Finally, health and system logs are something that should be provided by your cloud provider.  They make up a stream of events, such as application start, shutdown, scaling, web request tracing, and the results of periodic health checks.     When planning your monitoring strategy, you need to take into account how much information you\u2019ll be aggregating, the rate at which it comes in, and how much of it you\u2019re going to store. If your application dynamically scales from 1 instance to 100, that can also result in a hundredfold increase in your log traffic.  Auditing and monitoring cloud applications are often overlooked but are perhaps some of the most important things to plan and do properly for production deployments.  Getting telemetry done right can mean the difference between success and failure in the cloud.", 
            "title": "Telemetry"
        }, 
        {
            "location": "/telemetry/#kubernetes", 
            "text": "Liveliness/readiness probes  Prometheus  RED metrics (+ prebuild dashboards)", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/authn-and-authz/", 
            "text": "Authentication and Authorization\n\n\nSecurity is a vital part of any application and environment, and should never be an afterthought. A cloud-native application is a secure application, and with tools like OAuth2, OpenID Connect, various SSO servers and standards, as well as a near infinite supply of language-specific authentication and authorization libraries, security should be something that is baked into the application\u2019s development from day one, and not added as a bolt-on project after an application is running in production.\n\n\nEven if the only reason you implement security in your application is so you have an audit trail of which user made which data change, that alone is benefit enough to justify the relatively small amount of time and effort it takes to secure your application\u2019s endpoints.\n\n\nIn an ideal world, all applications would secure all of their endpoints with RBAC (role-based access control). Every request for an application\u2019s resources should know who is making the request, and the roles to which that consumer belongs. These roles dictate whether the calling client has sufficient permission for the application to honor the request. \nOr is ABAC a better answer?\n\n\n\n\nKubernetes\n\n\nCertificates\n\n\nHTTPS\n\n\nAPI Keys\n\n\nAPI Management Gateway", 
            "title": "Authentication and Authorization"
        }, 
        {
            "location": "/authn-and-authz/#authentication-and-authorization", 
            "text": "Security is a vital part of any application and environment, and should never be an afterthought. A cloud-native application is a secure application, and with tools like OAuth2, OpenID Connect, various SSO servers and standards, as well as a near infinite supply of language-specific authentication and authorization libraries, security should be something that is baked into the application\u2019s development from day one, and not added as a bolt-on project after an application is running in production.  Even if the only reason you implement security in your application is so you have an audit trail of which user made which data change, that alone is benefit enough to justify the relatively small amount of time and effort it takes to secure your application\u2019s endpoints.  In an ideal world, all applications would secure all of their endpoints with RBAC (role-based access control). Every request for an application\u2019s resources should know who is making the request, and the roles to which that consumer belongs. These roles dictate whether the calling client has sufficient permission for the application to honor the request.  Or is ABAC a better answer?   Kubernetes  Certificates  HTTPS  API Keys  API Management Gateway", 
            "title": "Authentication and Authorization"
        }
    ]
}